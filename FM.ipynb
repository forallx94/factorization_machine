{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# FM\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from sklearn.datasets import load_breast_cancer\r\n",
    "from sklearn.preprocessing import MinMaxScaler\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow import keras\r\n",
    "from tensorflow.keras.metrics import MeanAbsoluteError\r\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\r\n",
    "\r\n",
    "# GPU 확인\r\n",
    "tf.config.list_physical_devices('GPU')\r\n",
    "\r\n",
    "# 자료형 선언\r\n",
    "tf.keras.backend.set_floatx('float32')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "\r\n",
    "#### 마스터 데이터(상호 작용)\r\n",
    "#### 상호 작용(transaction) 마스터 데이터를 불러옴\r\n",
    "masterdf = pd.read_csv('./data/Transactions.csv')\r\n",
    "masterdf.head()\r\n",
    "\r\n",
    "### 데이터 정리 및 표준화를 위해 데이터 열 명칭 변경\r\n",
    "### 표준화는 병합의 용의성을 위해 열 명칭을 정렬\r\n",
    "masterdf.columns = ['Transaction ID', 'Customer ID', 'Transaction Date', 'Prod Subcat Code',\r\n",
    "       'Prod Cat Code', 'Qty', 'Rate', 'Tax', 'Total Amt', 'Store Type']\r\n",
    "\r\n",
    "###  상점 코드 타입을 숫자형으로 변경하여 새 열에 저장\r\n",
    "masterdf['Store Type Code'] = pd.factorize(masterdf['Store Type'])[0]\r\n",
    "masterdf.head(5)\r\n",
    "\r\n",
    "### quantity와 based price에서 총 순 매출액(Net sales) 계산 (도시마다의 세금이 다를 수 있어 세금 제외)\r\n",
    "masterdf['Net Sales'] = masterdf['Qty'] * masterdf['Rate']\r\n",
    "\r\n",
    "### category, subcategory, store type을 이용하여 고유한 material 표시기를 생성\r\n",
    "### 다른 sku는 다른 상점 유형에 판매된다고 가정\r\n",
    "masterdf['Material'] = masterdf['Prod Cat Code'].astype(str) + '-' + masterdf['Prod Subcat Code'].astype(str) + '-' + masterdf['Store Type'].astype(str)\r\n",
    "\r\n",
    "masterdf[['Customer ID','Material','Net Sales']]\r\n",
    "\r\n",
    "scaler = MinMaxScaler()\r\n",
    "grade = scaler.fit_transform(masterdf[['Net Sales']])\r\n",
    "\r\n",
    "grade = pd.DataFrame(grade,columns=['grade'])\r\n",
    "cust = pd.get_dummies(masterdf['Customer ID'],prefix='cust')\r\n",
    "item = pd.get_dummies(masterdf['Material'],prefix='item')\r\n",
    "\r\n",
    "df = pd.concat([cust,item,grade], axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# 데이터 로드\r\n",
    "X, Y = df[[i for i in df.columns if i!='grade']].to_numpy(), df[['grade']].to_numpy()\r\n",
    "X = X\r\n",
    "Y = Y\r\n",
    "\r\n",
    "n = X.shape[0]\r\n",
    "p = X.shape[1]\r\n",
    "\r\n",
    "k = 10\r\n",
    "batch_size = 32\r\n",
    "epochs = 100"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "X"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "Y"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.51009421],\n",
       "       [0.99798116],\n",
       "       [0.20349933],\n",
       "       ...,\n",
       "       [0.55693136],\n",
       "       [0.45168237],\n",
       "       [0.05074024]])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# batch train style"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "class FM(tf.keras.Model):\r\n",
    "    def __init__(self):\r\n",
    "        super(FM, self).__init__()\r\n",
    "\r\n",
    "        # 모델의 파라미터 정의\r\n",
    "        self.w_0 = tf.Variable([0.0])\r\n",
    "        self.w = tf.Variable(tf.zeros([p]))\r\n",
    "        self.V = tf.Variable(tf.random.normal(shape=(p, k)))\r\n",
    "\r\n",
    "    def call(self, inputs):\r\n",
    "        linear_terms = tf.reduce_sum(tf.math.multiply(self.w, inputs), axis=1)\r\n",
    "\r\n",
    "        interactions = 0.5 * tf.reduce_sum(\r\n",
    "            tf.math.pow(tf.matmul(inputs, self.V), 2)\r\n",
    "            - tf.matmul(tf.math.pow(inputs, 2), tf.math.pow(self.V, 2)),\r\n",
    "            1,\r\n",
    "            keepdims=False\r\n",
    "        )\r\n",
    "\r\n",
    "        y_hat = tf.math.sigmoid(self.w_0 + linear_terms + interactions)\r\n",
    "\r\n",
    "        return y_hat"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Forward\r\n",
    "def train_on_batch(model, optimizer, accuracy, inputs, targets):\r\n",
    "    with tf.GradientTape() as tape:\r\n",
    "        y_pred = model(inputs)\r\n",
    "        loss = tf.keras.losses.mean_squared_error(y_true=targets, y_pred=y_pred)\r\n",
    "    \r\n",
    "    # loss를 모델의 파라미터로 편미분하여 gradients를 구한다.\r\n",
    "    grads = tape.gradient(target=loss, sources=model.trainable_variables)\r\n",
    "\r\n",
    "    # apply_gradients()를 통해 processed gradients를 적용한다.\r\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n",
    "\r\n",
    "    # accuracy: update할 때마다 정확도는 누적되어 계산된다.\r\n",
    "    accuracy.update_state(targets, y_pred)\r\n",
    "\r\n",
    "    return loss\r\n",
    "\r\n",
    "\r\n",
    "# 반복 학습 함수\r\n",
    "def train(epochs):\r\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\r\n",
    "\r\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices(\r\n",
    "        (tf.cast(X_train, tf.float32), tf.cast(Y_train, tf.float32))).shuffle(500,seed = 42).batch(batch_size)\r\n",
    "\r\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices(\r\n",
    "        (tf.cast(X_test, tf.float32), tf.cast(Y_test, tf.float32))).shuffle(200,seed = 42).batch(batch_size)\r\n",
    "\r\n",
    "    model = FM()\r\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\r\n",
    "    accuracy = MeanAbsoluteError()\r\n",
    "    loss_history = []\r\n",
    "\r\n",
    "    for i in range(epochs):\r\n",
    "        for x, y in train_ds:\r\n",
    "            loss = train_on_batch(model, optimizer, accuracy, x, y)\r\n",
    "            if len(loss) == batch_size:\r\n",
    "                loss_history.append(loss)\r\n",
    "\r\n",
    "        if i % 2== 0:\r\n",
    "            print(\"스텝 {:03d}에서 누적 평균 손실: {:.4f}\".format(i, np.mean(loss_history)))\r\n",
    "            print(\"스텝 {:03d}에서 누적 정확도: {:.4f}\".format(i, accuracy.result().numpy()))\r\n",
    "\r\n",
    "    test_accuracy = MeanAbsoluteError()\r\n",
    "    for x, y in test_ds:\r\n",
    "        y_pred = model(x)\r\n",
    "        test_accuracy.update_state(y, y_pred)\r\n",
    "\r\n",
    "    print(\"테스트 정확도: {:.4f}\".format(test_accuracy.result().numpy()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "train(epochs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "스텝 000에서 누적 평균 손실: 0.1621\n",
      "스텝 000에서 누적 정확도: 0.3154\n",
      "스텝 002에서 누적 평균 손실: 0.1483\n",
      "스텝 002에서 누적 정확도: 0.2989\n",
      "스텝 004에서 누적 평균 손실: 0.1446\n",
      "스텝 004에서 누적 정확도: 0.2945\n",
      "스텝 006에서 누적 평균 손실: 0.1425\n",
      "스텝 006에서 누적 정확도: 0.2919\n",
      "스텝 008에서 누적 평균 손실: 0.1408\n",
      "스텝 008에서 누적 정확도: 0.2900\n",
      "스텝 010에서 누적 평균 손실: 0.1393\n",
      "스텝 010에서 누적 정확도: 0.2883\n",
      "스텝 012에서 누적 평균 손실: 0.1380\n",
      "스텝 012에서 누적 정확도: 0.2867\n",
      "스텝 014에서 누적 평균 손실: 0.1368\n",
      "스텝 014에서 누적 정확도: 0.2852\n",
      "스텝 016에서 누적 평균 손실: 0.1356\n",
      "스텝 016에서 누적 정확도: 0.2838\n",
      "스텝 018에서 누적 평균 손실: 0.1344\n",
      "스텝 018에서 누적 정확도: 0.2824\n",
      "스텝 020에서 누적 평균 손실: 0.1332\n",
      "스텝 020에서 누적 정확도: 0.2809\n",
      "스텝 022에서 누적 평균 손실: 0.1320\n",
      "스텝 022에서 누적 정확도: 0.2795\n",
      "스텝 024에서 누적 평균 손실: 0.1308\n",
      "스텝 024에서 누적 정확도: 0.2780\n",
      "스텝 026에서 누적 평균 손실: 0.1295\n",
      "스텝 026에서 누적 정확도: 0.2765\n",
      "스텝 028에서 누적 평균 손실: 0.1281\n",
      "스텝 028에서 누적 정확도: 0.2749\n",
      "스텝 030에서 누적 평균 손실: 0.1267\n",
      "스텝 030에서 누적 정확도: 0.2733\n",
      "스텝 032에서 누적 평균 손실: 0.1253\n",
      "스텝 032에서 누적 정확도: 0.2716\n",
      "스텝 034에서 누적 평균 손실: 0.1238\n",
      "스텝 034에서 누적 정확도: 0.2699\n",
      "스텝 036에서 누적 평균 손실: 0.1223\n",
      "스텝 036에서 누적 정확도: 0.2681\n",
      "스텝 038에서 누적 평균 손실: 0.1207\n",
      "스텝 038에서 누적 정확도: 0.2663\n",
      "스텝 040에서 누적 평균 손실: 0.1191\n",
      "스텝 040에서 누적 정확도: 0.2646\n",
      "스텝 042에서 누적 평균 손실: 0.1176\n",
      "스텝 042에서 누적 정확도: 0.2628\n",
      "스텝 044에서 누적 평균 손실: 0.1160\n",
      "스텝 044에서 누적 정확도: 0.2611\n",
      "스텝 046에서 누적 평균 손실: 0.1145\n",
      "스텝 046에서 누적 정확도: 0.2594\n",
      "스텝 048에서 누적 평균 손실: 0.1130\n",
      "스텝 048에서 누적 정확도: 0.2577\n",
      "스텝 050에서 누적 평균 손실: 0.1116\n",
      "스텝 050에서 누적 정확도: 0.2561\n",
      "스텝 052에서 누적 평균 손실: 0.1102\n",
      "스텝 052에서 누적 정확도: 0.2545\n",
      "스텝 054에서 누적 평균 손실: 0.1088\n",
      "스텝 054에서 누적 정확도: 0.2530\n",
      "스텝 056에서 누적 평균 손실: 0.1074\n",
      "스텝 056에서 누적 정확도: 0.2515\n",
      "스텝 058에서 누적 평균 손실: 0.1061\n",
      "스텝 058에서 누적 정확도: 0.2501\n",
      "스텝 060에서 누적 평균 손실: 0.1049\n",
      "스텝 060에서 누적 정확도: 0.2487\n",
      "스텝 062에서 누적 평균 손실: 0.1037\n",
      "스텝 062에서 누적 정확도: 0.2473\n",
      "스텝 064에서 누적 평균 손실: 0.1025\n",
      "스텝 064에서 누적 정확도: 0.2461\n",
      "스텝 066에서 누적 평균 손실: 0.1014\n",
      "스텝 066에서 누적 정확도: 0.2448\n",
      "스텝 068에서 누적 평균 손실: 0.1003\n",
      "스텝 068에서 누적 정확도: 0.2437\n",
      "스텝 070에서 누적 평균 손실: 0.0993\n",
      "스텝 070에서 누적 정확도: 0.2425\n",
      "스텝 072에서 누적 평균 손실: 0.0983\n",
      "스텝 072에서 누적 정확도: 0.2414\n",
      "스텝 074에서 누적 평균 손실: 0.0973\n",
      "스텝 074에서 누적 정확도: 0.2404\n",
      "스텝 076에서 누적 평균 손실: 0.0964\n",
      "스텝 076에서 누적 정확도: 0.2394\n",
      "스텝 078에서 누적 평균 손실: 0.0955\n",
      "스텝 078에서 누적 정확도: 0.2384\n",
      "스텝 080에서 누적 평균 손실: 0.0947\n",
      "스텝 080에서 누적 정확도: 0.2375\n",
      "스텝 082에서 누적 평균 손실: 0.0939\n",
      "스텝 082에서 누적 정확도: 0.2366\n",
      "스텝 084에서 누적 평균 손실: 0.0931\n",
      "스텝 084에서 누적 정확도: 0.2357\n",
      "스텝 086에서 누적 평균 손실: 0.0923\n",
      "스텝 086에서 누적 정확도: 0.2349\n",
      "스텝 088에서 누적 평균 손실: 0.0916\n",
      "스텝 088에서 누적 정확도: 0.2341\n",
      "스텝 090에서 누적 평균 손실: 0.0909\n",
      "스텝 090에서 누적 정확도: 0.2333\n",
      "스텝 092에서 누적 평균 손실: 0.0902\n",
      "스텝 092에서 누적 정확도: 0.2326\n",
      "스텝 094에서 누적 평균 손실: 0.0896\n",
      "스텝 094에서 누적 정확도: 0.2319\n",
      "스텝 096에서 누적 평균 손실: 0.0890\n",
      "스텝 096에서 누적 정확도: 0.2312\n",
      "스텝 098에서 누적 평균 손실: 0.0884\n",
      "스텝 098에서 누적 정확도: 0.2305\n",
      "테스트 정확도: 0.2032\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# layer train style"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## train"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "model_path = './model/FM.h5'\r\n",
    "batch_size = 256\r\n",
    "epochs = 700"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "inputs = keras.Input(shape=(X.shape[1],))\r\n",
    "outputs = FM()(inputs)\r\n",
    "model = keras.Model(inputs, outputs)\r\n",
    "\r\n",
    "# If there is a loss passed in `compile`, thee regularization\r\n",
    "# losses get added to it\r\n",
    "model.compile(optimizer=\"adam\", loss=\"mae\")\r\n",
    "\r\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\r\n",
    "\r\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\r\n",
    "mc = ModelCheckpoint(model_path, monitor='val_loss', mode='min', verbose=1, save_best_only=True)\r\n",
    "\r\n",
    "model.fit(X_train, Y_train,\r\n",
    "        batch_size=batch_size, \r\n",
    "        epochs=epochs, \r\n",
    "        shuffle=True, \r\n",
    "        callbacks=[es, mc], \r\n",
    "        validation_split=0.1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/700\n",
      "65/65 [==============================] - 1s 9ms/step - loss: 0.3932 - val_loss: 0.3878\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.38778, saving model to ./model\\FM.h5\n",
      "Epoch 2/700\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 0.3852 - val_loss: 0.3813\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.38778 to 0.38132, saving model to ./model\\FM.h5\n",
      "Epoch 3/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.3720 - val_loss: 0.3749\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.38132 to 0.37486, saving model to ./model\\FM.h5\n",
      "Epoch 4/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.3617 - val_loss: 0.3685\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.37486 to 0.36849, saving model to ./model\\FM.h5\n",
      "Epoch 5/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.3462 - val_loss: 0.3625\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.36849 to 0.36252, saving model to ./model\\FM.h5\n",
      "Epoch 6/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.3314 - val_loss: 0.3569\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.36252 to 0.35686, saving model to ./model\\FM.h5\n",
      "Epoch 7/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.3221 - val_loss: 0.3516\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35686 to 0.35162, saving model to ./model\\FM.h5\n",
      "Epoch 8/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.3193 - val_loss: 0.3464\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35162 to 0.34640, saving model to ./model\\FM.h5\n",
      "Epoch 9/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.3075 - val_loss: 0.3415\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.34640 to 0.34152, saving model to ./model\\FM.h5\n",
      "Epoch 10/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.2964 - val_loss: 0.3371\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.34152 to 0.33710, saving model to ./model\\FM.h5\n",
      "Epoch 11/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.2879 - val_loss: 0.3331\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.33710 to 0.33307, saving model to ./model\\FM.h5\n",
      "Epoch 12/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.2806 - val_loss: 0.3291\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.33307 to 0.32914, saving model to ./model\\FM.h5\n",
      "Epoch 13/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.2729 - val_loss: 0.3255\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.32914 to 0.32550, saving model to ./model\\FM.h5\n",
      "Epoch 14/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.2640 - val_loss: 0.3221\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.32550 to 0.32209, saving model to ./model\\FM.h5\n",
      "Epoch 15/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.2585 - val_loss: 0.3190\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.32209 to 0.31896, saving model to ./model\\FM.h5\n",
      "Epoch 16/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.2488 - val_loss: 0.3160\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.31896 to 0.31604, saving model to ./model\\FM.h5\n",
      "Epoch 17/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.2416 - val_loss: 0.3133\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.31604 to 0.31331, saving model to ./model\\FM.h5\n",
      "Epoch 18/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.2351 - val_loss: 0.3107\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.31331 to 0.31070, saving model to ./model\\FM.h5\n",
      "Epoch 19/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.2328 - val_loss: 0.3082\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.31070 to 0.30822, saving model to ./model\\FM.h5\n",
      "Epoch 20/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.2255 - val_loss: 0.3061\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.30822 to 0.30610, saving model to ./model\\FM.h5\n",
      "Epoch 21/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.2183 - val_loss: 0.3038\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.30610 to 0.30383, saving model to ./model\\FM.h5\n",
      "Epoch 22/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.2175 - val_loss: 0.3020\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.30383 to 0.30196, saving model to ./model\\FM.h5\n",
      "Epoch 23/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.2113 - val_loss: 0.3001\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.30196 to 0.30009, saving model to ./model\\FM.h5\n",
      "Epoch 24/700\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 0.2052 - val_loss: 0.2983\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.30009 to 0.29834, saving model to ./model\\FM.h5\n",
      "Epoch 25/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1994 - val_loss: 0.2968\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.29834 to 0.29680, saving model to ./model\\FM.h5\n",
      "Epoch 26/700\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 0.1986 - val_loss: 0.2952\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.29680 to 0.29520, saving model to ./model\\FM.h5\n",
      "Epoch 27/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1943 - val_loss: 0.2939\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.29520 to 0.29386, saving model to ./model\\FM.h5\n",
      "Epoch 28/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1874 - val_loss: 0.2925\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.29386 to 0.29248, saving model to ./model\\FM.h5\n",
      "Epoch 29/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1842 - val_loss: 0.2913\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.29248 to 0.29127, saving model to ./model\\FM.h5\n",
      "Epoch 30/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1830 - val_loss: 0.2901\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.29127 to 0.29006, saving model to ./model\\FM.h5\n",
      "Epoch 31/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1769 - val_loss: 0.2891\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.29006 to 0.28913, saving model to ./model\\FM.h5\n",
      "Epoch 32/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1779 - val_loss: 0.2881\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.28913 to 0.28812, saving model to ./model\\FM.h5\n",
      "Epoch 33/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1727 - val_loss: 0.2870\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.28812 to 0.28705, saving model to ./model\\FM.h5\n",
      "Epoch 34/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1692 - val_loss: 0.2862\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.28705 to 0.28620, saving model to ./model\\FM.h5\n",
      "Epoch 35/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1642 - val_loss: 0.2853\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.28620 to 0.28530, saving model to ./model\\FM.h5\n",
      "Epoch 36/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1633 - val_loss: 0.2843\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.28530 to 0.28429, saving model to ./model\\FM.h5\n",
      "Epoch 37/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1628 - val_loss: 0.2833\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.28429 to 0.28334, saving model to ./model\\FM.h5\n",
      "Epoch 38/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1584 - val_loss: 0.2828\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.28334 to 0.28284, saving model to ./model\\FM.h5\n",
      "Epoch 39/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1556 - val_loss: 0.2821\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.28284 to 0.28207, saving model to ./model\\FM.h5\n",
      "Epoch 40/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1540 - val_loss: 0.2815\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.28207 to 0.28145, saving model to ./model\\FM.h5\n",
      "Epoch 41/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1517 - val_loss: 0.2806\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.28145 to 0.28056, saving model to ./model\\FM.h5\n",
      "Epoch 42/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1466 - val_loss: 0.2799\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.28056 to 0.27995, saving model to ./model\\FM.h5\n",
      "Epoch 43/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1466 - val_loss: 0.2793\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.27995 to 0.27932, saving model to ./model\\FM.h5\n",
      "Epoch 44/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1424 - val_loss: 0.2787\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.27932 to 0.27870, saving model to ./model\\FM.h5\n",
      "Epoch 45/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1396 - val_loss: 0.2783\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.27870 to 0.27831, saving model to ./model\\FM.h5\n",
      "Epoch 46/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1355 - val_loss: 0.2776\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.27831 to 0.27764, saving model to ./model\\FM.h5\n",
      "Epoch 47/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1346 - val_loss: 0.2770\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.27764 to 0.27702, saving model to ./model\\FM.h5\n",
      "Epoch 48/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1332 - val_loss: 0.2766\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.27702 to 0.27656, saving model to ./model\\FM.h5\n",
      "Epoch 49/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1326 - val_loss: 0.2760\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.27656 to 0.27600, saving model to ./model\\FM.h5\n",
      "Epoch 50/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1324 - val_loss: 0.2755\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.27600 to 0.27554, saving model to ./model\\FM.h5\n",
      "Epoch 51/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1290 - val_loss: 0.2750\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.27554 to 0.27499, saving model to ./model\\FM.h5\n",
      "Epoch 52/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1272 - val_loss: 0.2748\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.27499 to 0.27476, saving model to ./model\\FM.h5\n",
      "Epoch 53/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1273 - val_loss: 0.2742\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.27476 to 0.27420, saving model to ./model\\FM.h5\n",
      "Epoch 54/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1238 - val_loss: 0.2739\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.27420 to 0.27386, saving model to ./model\\FM.h5\n",
      "Epoch 55/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1245 - val_loss: 0.2733\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.27386 to 0.27333, saving model to ./model\\FM.h5\n",
      "Epoch 56/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1186 - val_loss: 0.2730\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.27333 to 0.27302, saving model to ./model\\FM.h5\n",
      "Epoch 57/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1200 - val_loss: 0.2727\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.27302 to 0.27267, saving model to ./model\\FM.h5\n",
      "Epoch 58/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1169 - val_loss: 0.2725\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.27267 to 0.27249, saving model to ./model\\FM.h5\n",
      "Epoch 59/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1168 - val_loss: 0.2721\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.27249 to 0.27206, saving model to ./model\\FM.h5\n",
      "Epoch 60/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1146 - val_loss: 0.2718\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.27206 to 0.27179, saving model to ./model\\FM.h5\n",
      "Epoch 61/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1143 - val_loss: 0.2714\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.27179 to 0.27144, saving model to ./model\\FM.h5\n",
      "Epoch 62/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1123 - val_loss: 0.2711\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.27144 to 0.27115, saving model to ./model\\FM.h5\n",
      "Epoch 63/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1114 - val_loss: 0.2709\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.27115 to 0.27087, saving model to ./model\\FM.h5\n",
      "Epoch 64/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1076 - val_loss: 0.2707\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.27087 to 0.27065, saving model to ./model\\FM.h5\n",
      "Epoch 65/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1089 - val_loss: 0.2704\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.27065 to 0.27045, saving model to ./model\\FM.h5\n",
      "Epoch 66/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1047 - val_loss: 0.2701\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.27045 to 0.27013, saving model to ./model\\FM.h5\n",
      "Epoch 67/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1044 - val_loss: 0.2699\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.27013 to 0.26989, saving model to ./model\\FM.h5\n",
      "Epoch 68/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1042 - val_loss: 0.2697\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.26989 to 0.26965, saving model to ./model\\FM.h5\n",
      "Epoch 69/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1047 - val_loss: 0.2695\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.26965 to 0.26954, saving model to ./model\\FM.h5\n",
      "Epoch 70/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1031 - val_loss: 0.2693\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.26954 to 0.26932, saving model to ./model\\FM.h5\n",
      "Epoch 71/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1039 - val_loss: 0.2691\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.26932 to 0.26913, saving model to ./model\\FM.h5\n",
      "Epoch 72/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0967 - val_loss: 0.2689\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.26913 to 0.26892, saving model to ./model\\FM.h5\n",
      "Epoch 73/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0985 - val_loss: 0.2690\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.26892\n",
      "Epoch 74/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0982 - val_loss: 0.2686\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.26892 to 0.26865, saving model to ./model\\FM.h5\n",
      "Epoch 75/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0961 - val_loss: 0.2687\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.26865\n",
      "Epoch 76/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0958 - val_loss: 0.2681\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.26865 to 0.26809, saving model to ./model\\FM.h5\n",
      "Epoch 77/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0955 - val_loss: 0.2683\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.26809\n",
      "Epoch 78/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0936 - val_loss: 0.2681\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.26809 to 0.26807, saving model to ./model\\FM.h5\n",
      "Epoch 79/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0941 - val_loss: 0.2680\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.26807 to 0.26800, saving model to ./model\\FM.h5\n",
      "Epoch 80/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0907 - val_loss: 0.2678\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.26800 to 0.26779, saving model to ./model\\FM.h5\n",
      "Epoch 81/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0900 - val_loss: 0.2677\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.26779 to 0.26770, saving model to ./model\\FM.h5\n",
      "Epoch 82/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0913 - val_loss: 0.2676\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.26770 to 0.26764, saving model to ./model\\FM.h5\n",
      "Epoch 83/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0873 - val_loss: 0.2674\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.26764 to 0.26739, saving model to ./model\\FM.h5\n",
      "Epoch 84/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0886 - val_loss: 0.2673\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.26739 to 0.26726, saving model to ./model\\FM.h5\n",
      "Epoch 85/700\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 0.0872 - val_loss: 0.2672\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.26726 to 0.26716, saving model to ./model\\FM.h5\n",
      "Epoch 86/700\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 0.0881 - val_loss: 0.2672\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.26716\n",
      "Epoch 87/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0855 - val_loss: 0.2671\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.26716 to 0.26707, saving model to ./model\\FM.h5\n",
      "Epoch 88/700\n",
      "65/65 [==============================] - 0s 3ms/step - loss: 0.0847 - val_loss: 0.2671\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.26707 to 0.26706, saving model to ./model\\FM.h5\n",
      "Epoch 89/700\n",
      "65/65 [==============================] - 0s 3ms/step - loss: 0.0829 - val_loss: 0.2669\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.26706 to 0.26689, saving model to ./model\\FM.h5\n",
      "Epoch 90/700\n",
      "65/65 [==============================] - 0s 3ms/step - loss: 0.0825 - val_loss: 0.2668\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.26689 to 0.26683, saving model to ./model\\FM.h5\n",
      "Epoch 91/700\n",
      "65/65 [==============================] - 0s 3ms/step - loss: 0.0830 - val_loss: 0.2668\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.26683\n",
      "Epoch 92/700\n",
      "65/65 [==============================] - 0s 3ms/step - loss: 0.0819 - val_loss: 0.2667\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.26683 to 0.26674, saving model to ./model\\FM.h5\n",
      "Epoch 93/700\n",
      "65/65 [==============================] - 0s 3ms/step - loss: 0.0817 - val_loss: 0.2666\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.26674 to 0.26658, saving model to ./model\\FM.h5\n",
      "Epoch 94/700\n",
      "65/65 [==============================] - 0s 3ms/step - loss: 0.0797 - val_loss: 0.2666\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.26658 to 0.26656, saving model to ./model\\FM.h5\n",
      "Epoch 95/700\n",
      "65/65 [==============================] - 0s 3ms/step - loss: 0.0789 - val_loss: 0.2664\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.26656 to 0.26638, saving model to ./model\\FM.h5\n",
      "Epoch 96/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0792 - val_loss: 0.2664\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.26638\n",
      "Epoch 97/700\n",
      "65/65 [==============================] - 0s 3ms/step - loss: 0.0790 - val_loss: 0.2662\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.26638 to 0.26620, saving model to ./model\\FM.h5\n",
      "Epoch 98/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0771 - val_loss: 0.2663\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.26620\n",
      "Epoch 99/700\n",
      "65/65 [==============================] - 0s 3ms/step - loss: 0.0777 - val_loss: 0.2663\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.26620\n",
      "Epoch 100/700\n",
      "65/65 [==============================] - 0s 3ms/step - loss: 0.0773 - val_loss: 0.2664\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.26620\n",
      "Epoch 101/700\n",
      "65/65 [==============================] - 0s 3ms/step - loss: 0.0766 - val_loss: 0.2661\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.26620 to 0.26611, saving model to ./model\\FM.h5\n",
      "Epoch 102/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0765 - val_loss: 0.2662\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.26611\n",
      "Epoch 103/700\n",
      "65/65 [==============================] - 0s 3ms/step - loss: 0.0776 - val_loss: 0.2661\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.26611\n",
      "Epoch 104/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0752 - val_loss: 0.2663\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.26611\n",
      "Epoch 105/700\n",
      "65/65 [==============================] - 0s 3ms/step - loss: 0.0743 - val_loss: 0.2660\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.26611 to 0.26604, saving model to ./model\\FM.h5\n",
      "Epoch 106/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0726 - val_loss: 0.2662\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.26604\n",
      "Epoch 107/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0703 - val_loss: 0.2660\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.26604 to 0.26601, saving model to ./model\\FM.h5\n",
      "Epoch 108/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0705 - val_loss: 0.2661\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.26601\n",
      "Epoch 109/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0701 - val_loss: 0.2659\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.26601 to 0.26590, saving model to ./model\\FM.h5\n",
      "Epoch 110/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0715 - val_loss: 0.2659\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.26590 to 0.26588, saving model to ./model\\FM.h5\n",
      "Epoch 111/700\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 0.0693 - val_loss: 0.2659\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.26588\n",
      "Epoch 112/700\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 0.0699 - val_loss: 0.2658\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.26588 to 0.26577, saving model to ./model\\FM.h5\n",
      "Epoch 113/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0693 - val_loss: 0.2658\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.26577\n",
      "Epoch 114/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0689 - val_loss: 0.2657\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.26577 to 0.26572, saving model to ./model\\FM.h5\n",
      "Epoch 115/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0690 - val_loss: 0.2656\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.26572 to 0.26556, saving model to ./model\\FM.h5\n",
      "Epoch 116/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0685 - val_loss: 0.2656\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.26556\n",
      "Epoch 117/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0651 - val_loss: 0.2657\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.26556\n",
      "Epoch 118/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0653 - val_loss: 0.2655\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.26556 to 0.26554, saving model to ./model\\FM.h5\n",
      "Epoch 119/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0642 - val_loss: 0.2656\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.26554\n",
      "Epoch 120/700\n",
      "65/65 [==============================] - 0s 3ms/step - loss: 0.0660 - val_loss: 0.2654\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.26554 to 0.26543, saving model to ./model\\FM.h5\n",
      "Epoch 121/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0651 - val_loss: 0.2654\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.26543 to 0.26542, saving model to ./model\\FM.h5\n",
      "Epoch 122/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0651 - val_loss: 0.2653\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.26542 to 0.26526, saving model to ./model\\FM.h5\n",
      "Epoch 123/700\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 0.0657 - val_loss: 0.2655\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.26526\n",
      "Epoch 124/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0642 - val_loss: 0.2652\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.26526 to 0.26524, saving model to ./model\\FM.h5\n",
      "Epoch 125/700\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 0.0599 - val_loss: 0.2653\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.26524\n",
      "Epoch 126/700\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 0.0614 - val_loss: 0.2653\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.26524\n",
      "Epoch 127/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0615 - val_loss: 0.2654\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.26524\n",
      "Epoch 128/700\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 0.0602 - val_loss: 0.2650\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.26524 to 0.26504, saving model to ./model\\FM.h5\n",
      "Epoch 129/700\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 0.0622 - val_loss: 0.2652\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.26504\n",
      "Epoch 130/700\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 0.0607 - val_loss: 0.2651\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.26504\n",
      "Epoch 131/700\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 0.0611 - val_loss: 0.2652\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.26504\n",
      "Epoch 132/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0615 - val_loss: 0.2650\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.26504 to 0.26500, saving model to ./model\\FM.h5\n",
      "Epoch 133/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0602 - val_loss: 0.2650\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.26500\n",
      "Epoch 134/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0588 - val_loss: 0.2650\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.26500 to 0.26496, saving model to ./model\\FM.h5\n",
      "Epoch 135/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0574 - val_loss: 0.2650\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.26496\n",
      "Epoch 136/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0599 - val_loss: 0.2650\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.26496\n",
      "Epoch 137/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0575 - val_loss: 0.2649\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.26496 to 0.26488, saving model to ./model\\FM.h5\n",
      "Epoch 138/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0571 - val_loss: 0.2650\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.26488\n",
      "Epoch 139/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0570 - val_loss: 0.2650\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.26488\n",
      "Epoch 140/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0574 - val_loss: 0.2648\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.26488 to 0.26480, saving model to ./model\\FM.h5\n",
      "Epoch 141/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0547 - val_loss: 0.2648\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.26480 to 0.26475, saving model to ./model\\FM.h5\n",
      "Epoch 142/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0571 - val_loss: 0.2648\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.26475\n",
      "Epoch 143/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0561 - val_loss: 0.2647\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.26475 to 0.26471, saving model to ./model\\FM.h5\n",
      "Epoch 144/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0547 - val_loss: 0.2647\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.26471\n",
      "Epoch 145/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0552 - val_loss: 0.2646\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.26471 to 0.26456, saving model to ./model\\FM.h5\n",
      "Epoch 146/700\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 0.0531 - val_loss: 0.2646\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.26456\n",
      "Epoch 147/700\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 0.0538 - val_loss: 0.2645\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.26456 to 0.26447, saving model to ./model\\FM.h5\n",
      "Epoch 148/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0561 - val_loss: 0.2645\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.26447\n",
      "Epoch 149/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0558 - val_loss: 0.2646\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.26447\n",
      "Epoch 150/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0549 - val_loss: 0.2644\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.26447 to 0.26443, saving model to ./model\\FM.h5\n",
      "Epoch 151/700\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 0.0535 - val_loss: 0.2644\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.26443 to 0.26436, saving model to ./model\\FM.h5\n",
      "Epoch 152/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0525 - val_loss: 0.2644\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.26436\n",
      "Epoch 153/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0527 - val_loss: 0.2643\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.26436 to 0.26433, saving model to ./model\\FM.h5\n",
      "Epoch 154/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0528 - val_loss: 0.2643\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.26433\n",
      "Epoch 155/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0508 - val_loss: 0.2643\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.26433 to 0.26433, saving model to ./model\\FM.h5\n",
      "Epoch 156/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0523 - val_loss: 0.2642\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.26433 to 0.26421, saving model to ./model\\FM.h5\n",
      "Epoch 157/700\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 0.0503 - val_loss: 0.2642\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.26421 to 0.26419, saving model to ./model\\FM.h5\n",
      "Epoch 158/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0520 - val_loss: 0.2642\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.26419 to 0.26417, saving model to ./model\\FM.h5\n",
      "Epoch 159/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0503 - val_loss: 0.2642\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.26417 to 0.26415, saving model to ./model\\FM.h5\n",
      "Epoch 160/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0502 - val_loss: 0.2640\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.26415 to 0.26405, saving model to ./model\\FM.h5\n",
      "Epoch 161/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0506 - val_loss: 0.2641\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.26405\n",
      "Epoch 162/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0489 - val_loss: 0.2640\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.26405\n",
      "Epoch 163/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0489 - val_loss: 0.2641\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.26405\n",
      "Epoch 164/700\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0479 - val_loss: 0.2641\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.26405\n",
      "Epoch 00164: early stopping\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x203bf6237f0>"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "## model load\r\n",
    "inputs = keras.Input(shape=(X.shape[1],))\r\n",
    "outputs = FM()(inputs)\r\n",
    "model = keras.Model(inputs, outputs)\r\n",
    "model.load_weights(model_path)\r\n",
    "## Validation set Prediction\r\n",
    "y_pred=model.predict(X_test)\r\n",
    "\r\n",
    "tmp=[]\r\n",
    "for i in range(len(y_pred)):\r\n",
    "    tmp.append(abs(Y_test[i]-y_pred[i]))\r\n",
    "ANOMALY_SCORE=np.mean(tmp,axis=1)\r\n",
    "print(f'{ANOMALY_SCORE.mean()} test MAE')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.2651144966551641 test MAE\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## hyper paraneter tune"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for batch_size in [512,256,128,64,32,16,8]:\r\n",
    "    model_path = f'./model/FM_{batch_size}.h5'\r\n",
    "    epochs = 700\r\n",
    "\r\n",
    "    inputs = keras.Input(shape=(X.shape[1],))\r\n",
    "    outputs = FM()(inputs)\r\n",
    "    model = keras.Model(inputs, outputs)\r\n",
    "\r\n",
    "    # If there is a loss passed in `compile`, thee regularization\r\n",
    "    # losses get added to it\r\n",
    "    model.compile(optimizer=\"adam\", loss=\"mae\")\r\n",
    "\r\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\r\n",
    "\r\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\r\n",
    "    mc = ModelCheckpoint(model_path, monitor='val_loss', mode='min', verbose=1, save_best_only=True)\r\n",
    "\r\n",
    "    model.fit(X_train, Y_train,\r\n",
    "            batch_size=batch_size, \r\n",
    "            epochs=epochs, \r\n",
    "            shuffle=True, \r\n",
    "            callbacks=[es, mc], \r\n",
    "            validation_split=0.1)\r\n",
    "\r\n",
    "    ## model load\r\n",
    "    inputs = keras.Input(shape=(X.shape[1],))\r\n",
    "    outputs = FM()(inputs)\r\n",
    "    model = keras.Model(inputs, outputs)\r\n",
    "    model.load_weights(model_path)\r\n",
    "    ## Validation set Prediction\r\n",
    "    y_pred=model.predict(X_test)\r\n",
    "\r\n",
    "    tmp=[]\r\n",
    "    for i in range(len(y_pred)):\r\n",
    "        tmp.append(abs(Y_test[i]-y_pred[i]))\r\n",
    "    ANOMALY_SCORE=np.mean(tmp,axis=1)\r\n",
    "    print(f'{ANOMALY_SCORE.mean()} test MAE')\r\n",
    "    input()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('py38tf2': conda)"
  },
  "interpreter": {
   "hash": "f786b47412788e78e454191c2f87ff8c55e16bdd567362c780933370cac632e2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}